{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32a86cfae6018c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T00:32:10.838555Z",
     "start_time": "2025-05-20T00:32:06.916174Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dff607-2f32-4904-b061-5b552fb2d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„ loss å’Œ sparse_categorical_accuracy æ›²çº¿ã€‚\n",
    "    å‚æ•°ï¼š\n",
    "        history : tf.keras.callbacks.History å¯¹è±¡\n",
    "    \"\"\"\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    historydf[[\"loss\", \"val_loss\", \"sparse_categorical_accuracy\", \"val_sparse_categorical_accuracy\"]].plot(\n",
    "        ylim=(0, max(1.0, historydf.values.max())),\n",
    "        title=\"Training and Validation Loss / Accuracy\",\n",
    "        grid=True,\n",
    "        figsize=(10, 6)\n",
    "    )\n",
    "\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_acc = history.history['sparse_categorical_accuracy'][-1]\n",
    "    plt.title(f'Final Loss: {final_loss:.3f}, Final Accuracy: {final_acc:.3f}')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_multiple_histories_with_annotations(histories, labels=None, metric=\"sparse_categorical_accuracy\",\n",
    "                                             figsize=(14, 6), save_path=None, dpi=300, file_format=\"png\"):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶å¤šä¸ª history å¯¹è±¡çš„è®­ç»ƒ/éªŒè¯æ›²çº¿ï¼ˆæ”¯æŒä¸åŒé¢œè‰²/çº¿å‹ï¼Œæ ‡æ³¨æœ€é«˜ç‚¹ï¼Œå¯¼å‡ºå›¾åƒï¼‰\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "    - histories: list of tf.keras.callbacks.History objects\n",
    "    - labels: list of str, ç”¨äºæ ‡æ³¨æ¯ä¸ªæ¨¡å‹\n",
    "    - metric: str, è®­ç»ƒæŒ‡æ ‡åï¼Œå¦‚ \"sparse_categorical_accuracy\", \"loss\"\n",
    "    - figsize: tuple, å›¾åƒå°ºå¯¸\n",
    "    - save_path: str, æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆæ— æ‰©å±•åï¼‰\n",
    "    - dpi: int, å¯¼å‡ºå›¾åƒåˆ†è¾¨ç‡\n",
    "    - file_format: str, 'png' æˆ– 'svg'\n",
    "    \"\"\"\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"Model {i+1}\" for i in range(len(histories))]\n",
    "\n",
    "    colors = plt.cm.get_cmap('tab10', len(histories))  # ä¸åŒæ¨¡å‹ä¸åŒé¢œè‰²\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # === å­å›¾1ï¼šè®­ç»ƒæ›²çº¿ï¼ˆå®çº¿ï¼‰\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, (hist, label) in enumerate(zip(histories, labels)):\n",
    "        plt.plot(hist.epoch, hist.history[metric], linestyle='-', color=colors(i), label=f\"{label} (train)\")\n",
    "    plt.title(f\"Training {metric}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # === å­å›¾2ï¼šéªŒè¯æ›²çº¿ï¼ˆè™šçº¿ + æ ‡æ³¨ï¼‰\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, (hist, label) in enumerate(zip(histories, labels)):\n",
    "        val_metric = f\"val_{metric}\"\n",
    "        val_values = hist.history[val_metric]\n",
    "        epochs = hist.epoch\n",
    "        plt.plot(epochs, val_values, linestyle='--', color=colors(i), label=f\"{label} (val)\")\n",
    "\n",
    "        # è‡ªåŠ¨æ ‡æ³¨æœ€å¤§ val accuracy ä½ç½®\n",
    "        best_epoch = int(pd.Series(val_values).idxmax())\n",
    "        best_value = val_values[best_epoch]\n",
    "        plt.scatter(best_epoch, best_value, color=colors(i), marker='o')\n",
    "        plt.text(best_epoch, best_value + 0.01, f\"{best_value:.3f}\", fontsize=9, ha='center', color=colors(i))\n",
    "\n",
    "    plt.title(f\"Validation {metric}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # å¯¼å‡ºå›¾åƒ\n",
    "    if save_path:\n",
    "        full_path = f\"{save_path}.{file_format}\"\n",
    "        plt.savefig(full_path, dpi=dpi, format=file_format)\n",
    "        print(f\"âœ… å›¾åƒå·²ä¿å­˜ä¸º {full_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf62a5cc3f776da",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60eaf0e-4f0e-48a6-babf-cea234893c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization stats\n",
    "IMAGENET_MEAN = tf.constant([0.485, 0.456, 0.406], dtype=tf.float32)\n",
    "IMAGENET_STD = tf.constant([0.229, 0.224, 0.225], dtype=tf.float32)\n",
    "\n",
    "# Custom preprocessing function\n",
    "def preprocess_train(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 256, 256)  # çŸ­è¾¹ç¼©æ”¾åˆ°256ï¼Œpadé•¿è¾¹\n",
    "    image = tf.image.random_crop(image, size=(224, 224, 3))  # éšæœºè£å‰ª\n",
    "    image = tf.image.random_flip_left_right(image)  # éšæœºæ°´å¹³ç¿»è½¬\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)  # æ˜äº®åº¦æŠ–åŠ¨\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # å½’ä¸€åŒ–åˆ°0~1\n",
    "    image = (image - IMAGENET_MEAN) / IMAGENET_STD  # ä½¿ç”¨ImageNetå‡å€¼æ ‡å‡†åŒ–\n",
    "    return image, label\n",
    "\n",
    "def preprocess_val(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 256, 256)\n",
    "    image = tf.image.central_crop(image, central_fraction=0.875)  # è¿‘ä¼¼224 crop\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = (image - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return image, label\n",
    "\n",
    "# Load raw dataset\n",
    "raw_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"../data/CUB_200_2011/CUB_200_2011/images\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=(256, 256),  # åˆæ­¥resizeåˆ°ç»Ÿä¸€å°ºå¯¸ï¼ˆä¸ä½œå˜å½¢ï¼‰\n",
    "    batch_size=None,  # è¿”å›æœªæ‰¹é‡åŒ–çš„ (image, label), ä¿è¯ map æ—¶ä¼ å…¥çš„æ˜¯å•å¼ å›¾åƒ\n",
    "    shuffle=True,\n",
    "    seed=888\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "total_size = 11788  # CUB-200-2011 æ€»æ ·æœ¬æ•°\n",
    "train_size = 5994   # æŒ‰ç…§å®˜æ–¹ split\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_ds = raw_dataset.take(train_size).map(preprocess_train).batch(256).shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = raw_dataset.skip(train_size).map(preprocess_val).batch(256).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9978d60228b3a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T00:34:06.935141Z",
     "start_time": "2025-05-20T00:34:06.692025Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the dataset\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)  # (256, 224, 224, 3)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35452960bfe97464",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50dd41-58cd-49dd-a1a2-f553771b21e7",
   "metadata": {},
   "source": [
    "## Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3865f902d8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T00:34:31.619143Z",
     "start_time": "2025-05-20T00:34:29.383970Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101V2\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "def build_model():\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "    base_model = ResNet101V2(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
    "    base_model.trainable = True  # Full fine-tuning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # x = layers.Dropout(0.2)(x)  # optional\n",
    "\n",
    "    outputs = layers.Dense(200, activation=\"softmax\", name=\"Predictions\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.summary(show_trainable=True)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5911e-1987-456b-956d-ccc398987796",
   "metadata": {},
   "source": [
    "## Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02032b6d9a0334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T00:34:40.079873Z",
     "start_time": "2025-05-20T00:34:40.077182Z"
    }
   },
   "outputs": [],
   "source": [
    "def compile_model(model, lr=0.01, m=0.9, wd=0.0001):\n",
    "    optimizer = tf.keras.optimizers.SGD(\n",
    "        learning_rate=lr,\n",
    "        momentum=m,\n",
    "        weight_decay=wd  # TF â‰¥ 2.9 iff available, our tf.__version__ = 2.16\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce267ef04a30ccd8",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3e08c-2956-45d2-be9b-0d9a72437d8a",
   "metadata": {},
   "source": [
    "### step decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbce7c5-8281-450c-b3e3-4e11a395b186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step Decay LearningRateScheduler Functionï¼ˆLi et al., 2020ï¼‰\n",
    "def step_decay(epoch):\n",
    "    if epoch < 150:\n",
    "        return 0.01\n",
    "    elif epoch < 250:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17076bb-3eb8-411c-ad74-96c4ce6fa650",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4580a-2296-4024-b751-21b4167ace79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "\n",
    "checkpoint_path = \"checkpoints/best_model.keras\"\n",
    "log_dir = \"logs/single_run\"\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        mode=\"min\",\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    ),\n",
    "    lr_callback\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92957e-430e-4681-acc3-e0447929a568",
   "metadata": {},
   "source": [
    "# Output Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa28934-fb4b-4cb8-ad11-ba9d1547eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# output dir\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "base_ckpt_dir = f\"checkpoints_grid_{timestamp}\"\n",
    "base_log_dir = f\"logs_grid_{timestamp}\"\n",
    "os.makedirs(base_ckpt_dir, exist_ok=True)\n",
    "os.makedirs(base_log_dir, exist_ok=True)\n",
    "\n",
    "# result initial\n",
    "results = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b9131f-56c3-4ead-a093-513bd77b8564",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0e428-fb4c-4de6-9957-b7982e273389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T00:35:04.141805Z",
     "start_time": "2025-05-20T00:35:04.139227Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(lr=0.01, m=0.9, wd=0.0001, tag=None, epochs=1, use_early_stopping=True):\n",
    "    \"\"\"\n",
    "    æ„å»ºã€ç¼–è¯‘å¹¶è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒæŒ‡å®šè¶…å‚æ•°å’Œè¾“å‡ºè·¯å¾„ã€‚\n",
    "    \"\"\"\n",
    "    model = build_model()\n",
    "    model = compile_model(model, lr=lr, m=m, wd=wd)\n",
    "\n",
    "    if tag is None:\n",
    "        tag = f\"lr_{lr}_m_{m}\"\n",
    "\n",
    "    ckpt_dir = os.path.join(base_ckpt_dir, tag)\n",
    "    log_dir = os.path.join(base_log_dir, tag)\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(ckpt_dir, \"model_best.keras\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            mode=\"min\",\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True\n",
    "        ),\n",
    "        lr_callback\n",
    "    ]\n",
    "\n",
    "    if use_early_stopping:\n",
    "        callbacks.append(\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    best_val_loss = min(history.history[\"val_loss\"])\n",
    "    best_val_acc = max(history.history[\"val_sparse_categorical_accuracy\"])\n",
    "    best_epoch = history.history[\"val_loss\"].index(best_val_loss)\n",
    "\n",
    "    return {\n",
    "        \"learning_rate\": lr,\n",
    "        \"momentum\": m,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"best_val_accuracy\": best_val_acc,\n",
    "        \"epoch\": best_epoch,\n",
    "        \"history\": history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494f2c0-0ba0-439f-8f2e-5b9d2b712590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Parameter\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "momentums = [0.0, 0.8, 0.9, 0.95, 0.99]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for m in momentums:\n",
    "        print(f\\\"\\\\nğŸš€ Training: lr={lr}, m={m}\\\")\n",
    "        res = run_experiment(lr=lr, m=m, epochs=300, use_early_stopping=False)\n",
    "        results.append({k: res[k] for k in res if k != \\\"history\\\"})  # history å¯é€‰ä¿å­˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c7cfe-0275-455b-bc48-d8a69feee57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f\"grid_search_results_{timestamp}.csv\", index=False)\n",
    "\n",
    "# æ›²çº¿å›¾ 1ï¼šä¸åŒå­¦ä¹ ç‡ä¸‹ val accuracy æ›²çº¿ï¼ˆæŒ‰ momentum åˆ†ç»„ï¼‰\n",
    "plt.figure(figsize=(10, 6))\n",
    "for m in sorted(results_df['momentum'].unique()):\n",
    "    subset = results_df[results_df['momentum'] == m]\n",
    "    plt.plot(subset['learning_rate'], subset['best_val_accuracy'], marker='o', label=f'm={m}')\n",
    "\n",
    "plt.title(\"Validation Accuracy vs Learning Rate\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Best Validation Accuracy\")\n",
    "plt.xscale('log')  # å¯¹å­¦ä¹ ç‡ä½¿ç”¨ log åæ ‡æ›´ç›´è§‚\n",
    "plt.legend(title=\"Momentum\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "acc_curve_path = f\"val_accuracy_curve_{timestamp}.png\"\n",
    "plt.savefig(acc_curve_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"âœ… æ›²çº¿å›¾ï¼ˆAccuracyï¼‰å·²ä¿å­˜ä¸ºï¼š{acc_curve_path}\")\n",
    "\n",
    "\n",
    "# æ›²çº¿å›¾ 2ï¼šä¸åŒåŠ¨é‡ä¸‹ val accuracy æ›²çº¿ï¼ˆæŒ‰ learning_rate åˆ†ç»„ï¼‰\n",
    "plt.figure(figsize=(10, 6))\n",
    "for lr in sorted(results_df['learning_rate'].unique()):\n",
    "    subset = results_df[results_df['learning_rate'] == lr]\n",
    "    plt.plot(subset['momentum'], subset['best_val_accuracy'], marker='o', label=f'lr={lr}')\n",
    "\n",
    "plt.title(\"Validation Accuracy vs Momentum\")\n",
    "plt.xlabel(\"Momentum\")\n",
    "plt.ylabel(\"Best Validation Accuracy\")\n",
    "plt.legend(title=\"Learning Rate\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "momentum_curve_path = f\"val_accuracy_vs_momentum_curve_{timestamp}.png\"\n",
    "plt.savefig(momentum_curve_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"âœ… æ›²çº¿å›¾ï¼ˆvs Momentumï¼‰å·²ä¿å­˜ä¸ºï¼š{momentum_curve_path}\")\n",
    "\n",
    "# çƒ­åŠ›å›¾\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ä¿å­˜ CSV æ–‡ä»¶\n",
    "results_df = pd.DataFrame(results)\n",
    "csv_path = f\"grid_search_results_{timestamp}.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nâœ… æ‰€æœ‰å®éªŒå®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{csv_path}\")\n",
    "\n",
    "# === è¾“å‡º Heatmap å›¾åƒ ===\n",
    "\n",
    "# Pivot æˆçŸ©é˜µå½¢å¼\n",
    "acc_matrix = results_df.pivot(index=\"momentum\", columns=\"learning_rate\", values=\"best_val_accuracy\")\n",
    "loss_matrix = results_df.pivot(index=\"momentum\", columns=\"learning_rate\", values=\"best_val_loss\")\n",
    "\n",
    "# ç»˜åˆ¶ Accuracy çƒ­åŠ›å›¾\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(acc_matrix, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Validation Accuracy Heatmap\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Momentum\")\n",
    "plt.tight_layout()\n",
    "acc_fig_path = f\"val_accuracy_heatmap_{timestamp}.png\"\n",
    "plt.savefig(acc_fig_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"âœ… å‡†ç¡®ç‡å›¾åƒå·²ä¿å­˜ä¸ºï¼š{acc_fig_path}\")\n",
    "\n",
    "# ï¼ˆå¯é€‰ï¼‰ç»˜åˆ¶ Loss çƒ­åŠ›å›¾\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(loss_matrix, annot=True, fmt=\".3f\", cmap=\"Reds_r\")\n",
    "plt.title(\"Validation Loss Heatmap\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Momentum\")\n",
    "plt.tight_layout()\n",
    "loss_fig_path = f\"val_loss_heatmap_{timestamp}.png\"\n",
    "plt.savefig(loss_fig_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"âœ… æŸå¤±å›¾åƒå·²ä¿å­˜ä¸ºï¼š{loss_fig_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33c412-cc4c-4cda-a224-a882c5ead7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow M1)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
