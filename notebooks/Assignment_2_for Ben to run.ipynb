{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7bded2",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/6V836sX/DATA425-Labs/blob/main/Assignment_2_Summe_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ZYRAQcq-uPLL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYRAQcq-uPLL",
    "outputId": "d2c907f5-cbd0-4ddc-9c68-24d980a298d0"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # 挂载 Google Drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os, shutil, pathlib\n",
    "\n",
    "# src = \"/content/drive/My Drive/Colab Notebooks/DATA425A2/data/CUB_200_2011/images\"\n",
    "# dst = \"/content/images\"\n",
    "# if not pathlib.Path(dst).exists():  # 避免重复复制\n",
    "#     print(\"Copying images to local VM...\")\n",
    "#     shutil.copytree(src, dst)\n",
    "# else:\n",
    "#     print(\"Images already copied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9lYqaoQprdm4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lYqaoQprdm4",
    "outputId": "5be62d7d-b1b2-4432-d11e-4b0311e40ccc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# 设置基础路径\n",
    "base_dir = './'\n",
    "\n",
    "# 子路径\n",
    "# data_dir = '/content/images'\n",
    "data_dir = '../data/CUB_200_2011/images'\n",
    "base_ckpt_dir = os.path.join(base_dir, 'checkpoints')\n",
    "base_log_dir = os.path.join(base_dir, 'logs')\n",
    "base_result_dir = os.path.join(base_dir, 'results')\n",
    "notebook_dir = os.path.join(base_dir, 'notebook')\n",
    "\n",
    "# === 路径检查 ===\n",
    "# required_paths = {\n",
    "#     \"✅ Dataset path\": data_dir,\n",
    "#     \"📁 Checkpoint dir\": base_ckpt_dir,\n",
    "#     \"📁 Log dir\": base_log_dir,\n",
    "#     \"📁 Result dir\": base_result_dir,\n",
    "# }\n",
    "\n",
    "# # 检查 data_dir 是否存在（必须）\n",
    "# if not os.path.exists(data_dir):\n",
    "#     raise FileNotFoundError(f\"❌ 数据集路径不存在：{data_dir}\\n请检查是否已上传至 Google Drive 并命名正确。\")\n",
    "\n",
    "# print(\"🎉 数据集路径存在，开始创建保存目录...\")\n",
    "\n",
    "# # 其余路径：如果不存在则自动创建\n",
    "# for desc, path in required_paths.items():\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path, exist_ok=True)\n",
    "#         print(f\"{desc} 创建成功: {path}\")\n",
    "#     else:\n",
    "#         print(f\"{desc} 已存在: {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33dff607-2f32-4904-b061-5b552fb2d430",
   "metadata": {
    "id": "33dff607-2f32-4904-b061-5b552fb2d430"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "import os\n",
    "def plot_loss_accuracy(history):\n",
    "    \"\"\"\n",
    "    绘制训练过程中的 loss 和 sparse_categorical_accuracy 曲线。\n",
    "    参数：\n",
    "        history : tf.keras.callbacks.History 对象\n",
    "    \"\"\"\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    historydf[[\"loss\", \"val_loss\", \"sparse_categorical_accuracy\", \"val_sparse_categorical_accuracy\"]].plot(\n",
    "        ylim=(0, max(1.0, historydf.values.max())),\n",
    "        title=\"Training and Validation Loss / Accuracy\",\n",
    "        grid=True,\n",
    "        figsize=(10, 6)\n",
    "    )\n",
    "\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_acc = history.history['sparse_categorical_accuracy'][-1]\n",
    "    plt.title(f'Final Loss: {final_loss:.3f}, Final Accuracy: {final_acc:.3f}')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_multiple_histories_with_annotations(histories, labels=None, metric=\"sparse_categorical_accuracy\",\n",
    "                                             figsize=(14, 6), save_path=None, dpi=300, file_format=\"png\"):\n",
    "    \"\"\"\n",
    "    绘制多个 history 对象的训练/验证曲线（支持不同颜色/线型，标注最高点，导出图像）\n",
    "\n",
    "    参数：\n",
    "    - histories: list of tf.keras.callbacks.History objects\n",
    "    - labels: list of str, 用于标注每个模型\n",
    "    - metric: str, 训练指标名，如 \"sparse_categorical_accuracy\", \"loss\"\n",
    "    - figsize: tuple, 图像尺寸\n",
    "    - save_path: str, 文件保存路径（无扩展名）\n",
    "    - dpi: int, 导出图像分辨率\n",
    "    - file_format: str, 'png' 或 'svg'\n",
    "    \"\"\"\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"Model {i+1}\" for i in range(len(histories))]\n",
    "\n",
    "    colors = plt.cm.get_cmap('tab10', len(histories))  # 不同模型不同颜色\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # === 子图1：训练曲线（实线）\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, (hist, label) in enumerate(zip(histories, labels)):\n",
    "        plt.plot(hist.epoch, hist.history[metric], linestyle='-', color=colors(i), label=f\"{label} (train)\")\n",
    "    plt.title(f\"Training {metric}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # === 子图2：验证曲线（虚线 + 标注）\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, (hist, label) in enumerate(zip(histories, labels)):\n",
    "        val_metric = f\"val_{metric}\"\n",
    "        val_values = hist.history[val_metric]\n",
    "        epochs = hist.epoch\n",
    "        plt.plot(epochs, val_values, linestyle='--', color=colors(i), label=f\"{label} (val)\")\n",
    "\n",
    "        # 自动标注最大 val accuracy 位置\n",
    "        best_epoch = int(pd.Series(val_values).idxmax())\n",
    "        best_value = val_values[best_epoch]\n",
    "        plt.scatter(best_epoch, best_value, color=colors(i), marker='o')\n",
    "        plt.text(best_epoch, best_value + 0.01, f\"{best_value:.3f}\", fontsize=9, ha='center', color=colors(i))\n",
    "\n",
    "    plt.title(f\"Validation {metric}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 导出图像\n",
    "    if save_path:\n",
    "        full_path = f\"{save_path}.{file_format}\"\n",
    "        plt.savefig(full_path, dpi=dpi, format=file_format)\n",
    "        print(f\"✅ 图像已保存为 {full_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf62a5cc3f776da",
   "metadata": {
    "id": "adf62a5cc3f776da"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60eaf0e-4f0e-48a6-babf-cea234893c53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d60eaf0e-4f0e-48a6-babf-cea234893c53",
    "outputId": "7a4deb9d-160a-4e27-c452-babc5d80f248"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 16:10:44.218810: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-05-22 16:10:44.218831: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-05-22 16:10:44.218837: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-05-22 16:10:44.218852: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-22 16:10:44.218865: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11788 files belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "# ImageNet normalization stats\n",
    "IMAGENET_MEAN = tf.constant([0.485, 0.456, 0.406], dtype=tf.float32)\n",
    "IMAGENET_STD = tf.constant([0.229, 0.224, 0.225], dtype=tf.float32)\n",
    "\n",
    "# Custom preprocessing function\n",
    "def preprocess_train(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 256, 256)  # 短边缩放到256，pad长边\n",
    "    image = tf.image.random_crop(image, size=(224, 224, 3))  # 随机裁剪\n",
    "    image = tf.image.random_flip_left_right(image)  # 随机水平翻转\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)  # 明亮度抖动\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # 归一化到0~1\n",
    "    image = (image - IMAGENET_MEAN) / IMAGENET_STD  # 使用ImageNet均值标准化\n",
    "    return image, label\n",
    "\n",
    "def preprocess_val(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 256, 256)\n",
    "    image = tf.image.central_crop(image, central_fraction=0.875)  # 近似224 crop\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = (image - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return image, label\n",
    "\n",
    "# Load raw dataset\n",
    "raw_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    '../data/CUB_200_2011/CUB_200_2011/images',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=(256, 256),  # 初步resize到统一尺寸（不作变形）\n",
    "    batch_size=None,  # 返回未批量化的 (image, label), 保证 map 时传入的是单张图像\n",
    "    shuffle=True,\n",
    "    seed=888\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "total_size = 11788  # CUB-200-2011 总样本数\n",
    "train_size = 5994   # 按照官方 split\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_ds = raw_dataset.take(train_size).map(preprocess_train).batch(256).shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = raw_dataset.skip(train_size).map(preprocess_val).batch(256).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9978d60228b3a70",
   "metadata": {
    "id": "f9978d60228b3a70",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 16:10:56.956513: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 5 of 1000\n",
      "2025-05-22 16:11:08.329469: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 7 of 1000\n",
      "2025-05-22 16:11:21.152314: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 9 of 1000\n",
      "2025-05-22 16:11:39.700644: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 12 of 1000\n",
      "2025-05-22 16:11:51.067298: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 14 of 1000\n",
      "2025-05-22 16:12:09.123279: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 17 of 1000\n",
      "2025-05-22 16:12:19.526136: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 19 of 1000\n",
      "2025-05-22 16:12:38.412928: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 22 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 224, 224, 3)\n",
      "tf.Tensor(\n",
      "[ 59 197 122  26  62 142 180  57 134 135  21 116  41  68  86 198  45  51\n",
      "   8 198  13  61  90 158  77  28  73 165  63  27 101  87 104  34  45 118\n",
      "  98  33  75  12  15  81  14   3  69 128  33  14  90 155  54 122 108 188\n",
      " 128  45  19 194 123 124  29  47  69 126 148  70 127 146 178 159  78 107\n",
      " 196 147  53  68 121   8  10  44   8   3 170 104  21 189 119  98 128  34\n",
      "  77  84  76 146 126  28 112  46  58 147  53   8 110 139 112  65  32  92\n",
      " 111 128   7  37 150   3 138  44 109   2   1  97 101 115 178  24 133 172\n",
      "  71  75  31   8  13  85  87  41  24  28  81  71  19  72 167 194   3 143\n",
      " 138  71 118  33  87  45 138   8 129 157  15 124   1  38 197 129  42 163\n",
      "  27   8 166 110  11  71 154  43  29  26 103  41 122   4 189 151  52 158\n",
      "  43  85 100  71  37  18  95 123 152  37  58 187  10 168  66  30 140 166\n",
      "  22  58 188 146   2  94 190  70  39  29  76  10  38 142  51  33 188   5\n",
      " 111 107 173 137  92 126  20  14 115  86  24  96  53 171 153 144  72 173\n",
      " 115 136 141  72  34 178  89  54  35 128  82 190  77 194  51   0  40 149\n",
      " 132  19 125 171], shape=(256,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 16:12:47.217464: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "2025-05-22 16:12:47.371731: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)  # (256, 224, 224, 3)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35452960bfe97464",
   "metadata": {
    "id": "35452960bfe97464"
   },
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50dd41-58cd-49dd-a1a2-f553771b21e7",
   "metadata": {
    "id": "6b50dd41-58cd-49dd-a1a2-f553771b21e7"
   },
   "source": [
    "## Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d3865f902d8c8",
   "metadata": {
    "id": "57d3865f902d8c8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101V2\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "def build_model():\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "    base_model = ResNet101V2(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
    "    base_model.trainable = True  # Full fine-tuning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # x = layers.Dropout(0.2)(x)  # optional\n",
    "\n",
    "    outputs = layers.Dense(200, activation=\"softmax\", name=\"Predictions\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # model.summary(show_trainable=True)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5911e-1987-456b-956d-ccc398987796",
   "metadata": {
    "id": "c3d5911e-1987-456b-956d-ccc398987796"
   },
   "source": [
    "## Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb02032b6d9a0334",
   "metadata": {
    "id": "eb02032b6d9a0334"
   },
   "outputs": [],
   "source": [
    "def compile_model(model, lr=0.01, m=0.9, wd=0.0001):\n",
    "    optimizer = tf.keras.optimizers.SGD(\n",
    "        learning_rate=lr,\n",
    "        momentum=m,\n",
    "        weight_decay=wd  # TF ≥ 2.9 iff available, our tf.__version__ = 2.16\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce267ef04a30ccd8",
   "metadata": {
    "id": "ce267ef04a30ccd8"
   },
   "source": [
    "## Model Fitting Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3e08c-2956-45d2-be9b-0d9a72437d8a",
   "metadata": {
    "id": "c9d3e08c-2956-45d2-be9b-0d9a72437d8a"
   },
   "source": [
    "### step decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbbce7c5-8281-450c-b3e3-4e11a395b186",
   "metadata": {
    "id": "cbbce7c5-8281-450c-b3e3-4e11a395b186",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step Decay LearningRateScheduler Function（Li et al., 2020）\n",
    "def step_decay(epoch):\n",
    "    if epoch < 150:\n",
    "        return 0.01\n",
    "    elif epoch < 250:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17076bb-3eb8-411c-ad74-96c4ce6fa650",
   "metadata": {
    "id": "c17076bb-3eb8-411c-ad74-96c4ce6fa650"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa4580a-2296-4024-b751-21b4167ace79",
   "metadata": {
    "id": "eaa4580a-2296-4024-b751-21b4167ace79"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "\n",
    "checkpoint_path = \"checkpoints/best_model.keras\"\n",
    "log_dir = \"logs/single_run\"\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        mode=\"min\",\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    ),\n",
    "    lr_callback\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92957e-430e-4681-acc3-e0447929a568",
   "metadata": {
    "id": "da92957e-430e-4681-acc3-e0447929a568"
   },
   "source": [
    "## Output Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa28934-fb4b-4cb8-ad11-ba9d1547eb76",
   "metadata": {
    "id": "9aa28934-fb4b-4cb8-ad11-ba9d1547eb76"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# output dir\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "base_ckpt_dir = f\"checkpoints_grid_{timestamp}\"\n",
    "base_log_dir = f\"logs_grid_{timestamp}\"\n",
    "os.makedirs(base_ckpt_dir, exist_ok=True)\n",
    "os.makedirs(base_log_dir, exist_ok=True)\n",
    "\n",
    "# result initial\n",
    "results = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b9131f-56c3-4ead-a093-513bd77b8564",
   "metadata": {
    "id": "d2b9131f-56c3-4ead-a093-513bd77b8564"
   },
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "XU52SYC97Bm-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU52SYC97Bm-",
    "outputId": "2680e0eb-7835-4640-c1ed-1d1b18b745f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494f2c0-0ba0-439f-8f2e-5b9d2b712590",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5494f2c0-0ba0-439f-8f2e-5b9d2b712590",
    "outputId": "f0d70461-830c-4e1a-caa1-9269f38de24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 [Exp1] Training: wd=0.0001, m=0.0\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 16:12:52.560727: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-05-22 16:13:08.739828: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 19 of 1000\n",
      "2025-05-22 16:13:20.764609: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 23 of 1000\n",
      "2025-05-22 16:13:22.062016: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "/Users/summersmac/miniforge3/envs/tf_env/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 6.56398, saving model to checkpoints_grid_20250522-1612/exp1_lr0.01_wd0.0001_m0.0/best_model.keras\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 20s/step - val_accuracy: 0.0036 - val_loss: 6.5640\n",
      "Epoch 2/30\n"
     ]
    }
   ],
   "source": [
    "# ✅ 实验图 1：Validation Error vs Momentum（对比不同 λ）\n",
    "def experiment_vary_momentum_vs_weight_decay():\n",
    "    learning_rate = 0.01\n",
    "    weight_decays = [0.0001, 0.0]\n",
    "    momentums = [0.0, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for wd in weight_decays:\n",
    "        for m in momentums:\n",
    "            tag = f\"exp1_lr{learning_rate}_wd{wd}_m{m}\"\n",
    "            print(f\"\\n🚀 [Exp1] Training: wd={wd}, m={m}\")\n",
    "\n",
    "            result = run_experiment(\n",
    "                lr=learning_rate,\n",
    "                m=m,\n",
    "                wd=wd,\n",
    "                tag=tag,\n",
    "                epochs=30,\n",
    "                use_early_stopping=False,\n",
    "                train_ds=train_ds,\n",
    "                val_ds=val_ds\n",
    "            )\n",
    "            result['weight_decay'] = wd\n",
    "            results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ✅ 实验图 2：Validation Error vs Learning Rate（对比不同 momentum）\n",
    "def experiment_vary_lr_vs_momentum():\n",
    "    weight_decay = 0.0001\n",
    "    learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    momentums = [0.9, 0.0]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for m in momentums:\n",
    "        for lr in learning_rates:\n",
    "            tag = f\"exp2_m{m}_lr{lr}_wd{weight_decay}\"\n",
    "            print(f\"\\n🚀 [Exp2] Training: m={m}, lr={lr}\")\n",
    "\n",
    "            result = run_experiment(\n",
    "                lr=lr,\n",
    "                m=m,\n",
    "                wd=weight_decay,\n",
    "                tag=tag,\n",
    "                epochs=30,\n",
    "                use_early_stopping=False,\n",
    "                train_ds=train_ds,\n",
    "                val_ds=val_ds\n",
    "            )\n",
    "            result['momentum'] = m\n",
    "            results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ✅ run_experiment 完整定义（支持传入数据）\n",
    "def run_experiment(lr=0.01, m=0.9, wd=0.0001, tag=None, epochs=30, \n",
    "                   use_early_stopping=True, train_ds=None, val_ds=None):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    import os\n",
    "\n",
    "    model = build_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=m, weight_decay=wd)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    callbacks = []\n",
    "    if tag:\n",
    "        ckpt_dir = os.path.join(base_ckpt_dir, tag)\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        callbacks.append(keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(ckpt_dir, \"best_model.keras\"),\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            verbose=1\n",
    "        ))\n",
    "    if use_early_stopping:\n",
    "        callbacks.append(keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=0))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    best_val_acc = max(history.history[\"val_accuracy\"])\n",
    "    best_val_loss = min(history.history[\"val_loss\"])\n",
    "    best_epoch = history.history[\"val_loss\"].index(best_val_loss)\n",
    "\n",
    "    return {\n",
    "        \"learning_rate\": lr,\n",
    "        \"momentum\": m,\n",
    "        \"weight_decay\": wd,\n",
    "        \"best_val_accuracy\": best_val_acc,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"epoch\": best_epoch\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ 绘图函数：Line Plot 并标注 min top-1（用于 df_exp1 或 df_exp2）\n",
    "def plot_validation_error_line(df, x_var, group_var, title, xlabel, output_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    grouped = df.groupby(group_var)\n",
    "    for name, group in grouped:\n",
    "        x = group[x_var]\n",
    "        y = 100 * (1 - group['best_val_accuracy'])  # 转换为 Validation Error\n",
    "        plt.plot(x, y, marker='o', label=f\"{group_var}={name}\")\n",
    "\n",
    "        # 标出 min point\n",
    "        min_idx = y.idxmin()\n",
    "        min_x = x[min_idx]\n",
    "        min_y = y[min_idx]\n",
    "        min_top1 = min_y\n",
    "        plt.annotate(f\"min top1={min_top1:.2f}\",\n",
    "                     (min_x, min_y),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0,10),\n",
    "                     ha='center')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Validation Error\")\n",
    "    if x_var == \"learning_rate\":\n",
    "        plt.xscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ✅ 运行示例：\n",
    "df_exp1 = experiment_vary_momentum_vs_weight_decay()\n",
    "df_exp1.to_csv(\"exp1.csv\", index=False)\n",
    "plot_validation_error_line(df_exp1, x_var=\"momentum\", group_var=\"weight_decay\",\n",
    "                            title=\"birds, imagenet, η=0.01, n=64\",\n",
    "                            xlabel=\"Momentum m\", output_path=\"exp1_val_error.png\")\n",
    "\n",
    "df_exp2 = experiment_vary_lr_vs_momentum()\n",
    "df_exp2.to_csv(\"exp2.csv\", index=False)\n",
    "plot_validation_error_line(df_exp2, x_var=\"learning_rate\", group_var=\"momentum\",\n",
    "                            title=\"birds, imagenet, λ=0.0001, n=64\",\n",
    "                            xlabel=\"learning rate η\", output_path=\"exp2_val_error.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c7cfe-0275-455b-bc48-d8a69feee57e",
   "metadata": {
    "id": "3d2c7cfe-0275-455b-bc48-d8a69feee57e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33c412-cc4c-4cda-a224-a882c5ead7b5",
   "metadata": {
    "id": "3a33c412-cc4c-4cda-a224-a882c5ead7b5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (TensorFlow M1)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
