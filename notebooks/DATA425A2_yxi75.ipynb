{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7644aad4-9ace-4242-a84f-8157219aa92f",
   "metadata": {},
   "source": [
    "# Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8860ef-6edc-4aad-bfef-c31dc15026ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==== 路径参数 ====\n",
    "CUB_ROOT = '/content/drive/My Drive/Colab Notebooks/DATA425A2/data/CUB_200_2011'\n",
    "IMG_ROOT = os.path.join(CUB_ROOT, 'images')\n",
    "CLASSES_TXT = os.path.join(CUB_ROOT, 'classes.txt')\n",
    "SPLIT_TXT = os.path.join(CUB_ROOT, 'train_test_split.txt')\n",
    "IMGLIST_TXT = os.path.join(CUB_ROOT, 'images.txt')\n",
    "\n",
    "# 你的实验目录\n",
    "DRIVE_EXP_DIR = \"/content/drive/My Drive/Colab Notebooks/DATA425A2\"\n",
    "os.makedirs(DRIVE_EXP_DIR, exist_ok=True)\n",
    "result_path1 = os.path.join(DRIVE_EXP_DIR, \"exp1_results.csv\")\n",
    "result_path2 = os.path.join(DRIVE_EXP_DIR, \"exp2_results.csv\")\n",
    "models_dir = os.path.join(DRIVE_EXP_DIR, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "NUM_CLASSES = 200\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "\n",
    "# ==== 读取图片id到路径 ====\n",
    "imgid2name = {}\n",
    "with open(IMGLIST_TXT, \"r\") as f:\n",
    "    for line in f:\n",
    "        idx, relpath = line.strip().split()\n",
    "        imgid2name[int(idx)] = relpath\n",
    "\n",
    "# ==== 类别名到id ====\n",
    "class_name2id = {}\n",
    "with open(CLASSES_TXT, \"r\") as f:\n",
    "    for line in f:\n",
    "        idx, name = line.strip().split()\n",
    "        class_name2id[name] = int(idx) - 1\n",
    "\n",
    "# ==== 官方split分组 ====\n",
    "train_imglist, val_imglist = [], []\n",
    "with open(SPLIT_TXT, \"r\") as f:\n",
    "    for line in f:\n",
    "        idx, is_train = line.strip().split()\n",
    "        img_relpath = imgid2name[int(idx)]\n",
    "        class_folder = img_relpath.split('/')[0]\n",
    "        label = class_name2id[class_folder]\n",
    "        full_path = os.path.join(IMG_ROOT, img_relpath)\n",
    "        if int(is_train) == 1:\n",
    "            train_imglist.append((full_path, label))\n",
    "        else:\n",
    "            val_imglist.append((full_path, label))\n",
    "\n",
    "print(f\"Train imgs: {len(train_imglist)}, Val imgs: {len(val_imglist)}\")  # 5994, 5794\n",
    "\n",
    "IMAGENET_MEAN = tf.constant([0.485, 0.456, 0.406], dtype=tf.float32)\n",
    "IMAGENET_STD = tf.constant([0.229, 0.224, 0.225], dtype=tf.float32)\n",
    "\n",
    "def preprocess_train(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 256, 256)\n",
    "    image = tf.image.random_crop(image, size=(224, 224, 3))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = (image - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return image, label\n",
    "\n",
    "def preprocess_val(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 256, 256)\n",
    "    image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = (image - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return image, label\n",
    "\n",
    "def decode_img(filename, label):\n",
    "    try:\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        return img, label\n",
    "    except:\n",
    "        # print(f\"Decode failed: {filename}\")  # 如需debug可打开\n",
    "        return tf.zeros([256,256,3], dtype=tf.float32), label\n",
    "\n",
    "def build_dataset(img_label_list, batch_size, is_train=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((\n",
    "        [p for p, l in img_label_list],\n",
    "        [l for p, l in img_label_list]\n",
    "    ))\n",
    "    ds = ds.map(decode_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_train:\n",
    "        ds = ds.map(preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        ds = ds.shuffle(1000)\n",
    "    else:\n",
    "        ds = ds.map(preprocess_val, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = build_dataset(train_imglist, batch_size=BATCH_SIZE, is_train=True)\n",
    "val_ds = build_dataset(val_imglist, batch_size=BATCH_SIZE, is_train=False)\n",
    "\n",
    "for batch in train_ds.take(1):\n",
    "    images, labels = batch\n",
    "    print(\"Train batch:\", images.shape, labels.shape)\n",
    "for batch in val_ds.take(1):\n",
    "    images, labels = batch\n",
    "    print(\"Val batch:\", images.shape, labels.shape)\n",
    "\n",
    "def run_experiment(momentum=0.9, lr=0.01, wd=0.0001, exp_tag=\"\"):\n",
    "    # 每次实验动态拼接模型路径\n",
    "    model_path = os.path.join(models_dir, f\"{exp_tag}.weights.h5\")\n",
    "    base = tf.keras.applications.ResNet101V2(include_top=False, weights=\"imagenet\", input_shape=(224,224,3), pooling=\"avg\")\n",
    "    base.trainable = True\n",
    "    model = tf.keras.Sequential([\n",
    "        base,\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "    ])\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            layer.kernel_regularizer = tf.keras.regularizers.l2(wd)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor=\"val_accuracy\", save_best_only=True, save_weights_only=True, verbose=1\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint]\n",
    "    )\n",
    "    val_acc_last = history.history[\"val_accuracy\"][-1]  # report the Top-1 validation error at the end of training\n",
    "    val_error_last = 1 - val_acc_last\n",
    "\n",
    "    val_error_best = 1 - max(history.history[\"val_accuracy\"])\n",
    "    val_acc_best   = max(history.history[\"val_accuracy\"])\n",
    "    \n",
    "    result = {\n",
    "        \"exp_tag\": exp_tag,\n",
    "        \"momentum\": momentum,\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": wd,\n",
    "        \"val_error_best\": best_val_error,\n",
    "        \"val_acc_best\": best_val_acc,\n",
    "        \"val_error_last\": history.history[\"loss\"][-1],\n",
    "        \"val_acc_last\": history.history[\"accuracy\"][-1],\n",
    "        \"model_path\": model_path\n",
    "    }\n",
    "    return result, history\n",
    "\n",
    "def experiment1():\n",
    "    lrs = [0.01]\n",
    "    mms = [0.0, 0.8, 0.9, 0.95, 0.99]\n",
    "    wds = [0.0, 0.0001]\n",
    "    results = []\n",
    "    if os.path.exists(result_path1):\n",
    "        done_df = pd.read_csv(result_path1)\n",
    "        done_tags = set(done_df[\"exp_tag\"])\n",
    "        results.extend(done_df.to_dict(\"records\"))\n",
    "    else:\n",
    "        done_tags = set()\n",
    "    for wd in wds:\n",
    "        for mm in mms:\n",
    "            for lr in lrs:\n",
    "                tag = f\"exp1_wd{wd}_m{mm}_lr{lr}\"\n",
    "                if tag in done_tags:\n",
    "                    print(f\"跳过已完成：{tag}\")\n",
    "                    continue\n",
    "                print(f\"\\n==== Running {tag} ====\")\n",
    "                res, _ = run_experiment(momentum=mm, lr=lr, wd=wd, exp_tag=tag)\n",
    "                results.append(res)\n",
    "                pd.DataFrame(results).to_csv(result_path1, index=False)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def experiment2():\n",
    "    mms = [0.0, 0.9]\n",
    "    lrs = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    wd = 0.0001\n",
    "    results = []\n",
    "    if os.path.exists(result_path2):\n",
    "        done_df = pd.read_csv(result_path2)\n",
    "        done_tags = set(done_df[\"exp_tag\"])\n",
    "        results.extend(done_df.to_dict(\"records\"))\n",
    "    else:\n",
    "        done_tags = set()\n",
    "    for mm in mms:\n",
    "        for lr in lrs:\n",
    "            tag = f\"exp2_m{mm}_lr{lr}\"\n",
    "            if tag in done_tags:\n",
    "                print(f\"跳过已完成：{tag}\")\n",
    "                continue\n",
    "            print(f\"\\n==== Running {tag} ====\")\n",
    "            res, _ = run_experiment(momentum=mm, lr=lr, wd=wd, exp_tag=tag)\n",
    "            results.append(res)\n",
    "            pd.DataFrame(results).to_csv(result_path2, index=False)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running Experiment 1 ...\")\n",
    "    df1 = experiment1()\n",
    "    print(\"Exp1 complete. Results in exp1_results.csv\")\n",
    "    print(\"Running Experiment 2 ...\")\n",
    "    df2 = experiment2()\n",
    "    print(\"Exp2 complete. Results in exp2_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecf755-51a8-43a1-b2e6-84f1339367a4",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006b8e2-598d-400f-9f50-9d48ea7867db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DRIVE_EXP_DIR = \"../figures\"\n",
    "result_path1 = os.path.join(DRIVE_EXP_DIR, \"exp1_results.csv\")\n",
    "result_path2 = os.path.join(DRIVE_EXP_DIR, \"exp2_results.csv\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_exp1(df, save_path=None):\n",
    "    x_vals = sorted(df['momentum'].unique())\n",
    "    color_map = plt.get_cmap('Set1')  # \n",
    "    fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    lines = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, wd in enumerate(sorted(df['weight_decay'].unique())):\n",
    "        dfg = df[df['weight_decay']==wd].sort_values('momentum')\n",
    "        color = color_map(idx)\n",
    "        # 主曲线: best_val_error\n",
    "        line1, = ax1.plot(dfg['momentum'], dfg['val_error_last'], '-o', label=f'λ={wd}', color=color)\n",
    "        # 数据标签\n",
    "        # for xm, yerr in zip(dfg['momentum'], dfg['best_val_error']):\n",
    "        #     ax1.annotate(f\"{yerr:.2f}\", (xm, yerr), textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=9, color=color)\n",
    "        # 次曲线: best_val_acc\n",
    "        line2, = ax2.plot(dfg['momentum'], dfg['val_acc_last'], '--s', label=f'λ={wd} acc', color=color, alpha=0.6)\n",
    "        # for xm, yacc in zip(dfg['momentum'], dfg['best_val_acc']):\n",
    "        #     ax2.annotate(f\"{yacc:.2f}\", (xm, yacc), textcoords=\"offset points\", xytext=(0,-12), ha='center', fontsize=9, color=color)\n",
    "        lines.append(line1)\n",
    "        labels.append(f'val error λ={wd}')\n",
    "        lines.append(line2)\n",
    "        labels.append(f'val acc λ={wd}')\n",
    "\n",
    "    ax1.set_xticks(x_vals)\n",
    "    ax1.set_xlabel(\"Momentum $m$\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Best Validation Error $1-\\\\mathrm{acc}$\", fontsize=12, color='tab:blue')\n",
    "    ax2.set_ylabel(\"Best Validation Accuracy\", fontsize=12, color='tab:orange')\n",
    "    ax1.set_title(\n",
    "        \"Experiment 1 (CUB-200-2011 birds)\\n\"\n",
    "        \"ResNet101V2 (ImageNet), lr = 0.01 (fixed), batch size=64\\n\"\n",
    "    )\n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.legend(lines, labels, loc='center left', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_exp2(df, save_path=None):\n",
    "    x_vals = sorted(df['lr'].unique())\n",
    "    color_map = plt.get_cmap('Set1')\n",
    "    fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    lines = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, m in enumerate(sorted(df['momentum'].unique())):\n",
    "        dfg = df[df['momentum']==m].sort_values('lr')\n",
    "        color = color_map(idx)\n",
    "        # 主曲线: best_val_error\n",
    "        line1, = ax1.plot(dfg['lr'], dfg['val_error_last'], '-o', label=f'm={m}', color=color)\n",
    "        # for xl, yerr in zip(dfg['lr'], dfg['best_val_error']):\n",
    "        #     ax1.annotate(f\"{yerr:.2f}\", (xl, yerr), textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=9, color=color)\n",
    "        # 次曲线: best_val_acc\n",
    "        line2, = ax2.plot(dfg['lr'], dfg['val_acc_last'], '--s', label=f'm={m} acc', color=color, alpha=0.6)\n",
    "        # for xl, yacc in zip(dfg['lr'], dfg['best_val_acc']):\n",
    "        #     ax2.annotate(f\"{yacc:.2f}\", (xl, yacc), textcoords=\"offset points\", xytext=(0,-12), ha='center', fontsize=9, color=color)\n",
    "        lines.append(line1)\n",
    "        labels.append(f'val error m={m}')\n",
    "        lines.append(line2)\n",
    "        labels.append(f'val acc m={m}')\n",
    "\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xticks(x_vals)\n",
    "    ax1.get_xaxis().set_major_formatter(plt.ScalarFormatter())\n",
    "    ax1.set_xlabel(\"Learning Rate $\\\\eta$\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Best Validation Error $1-\\\\mathrm{acc}$\", fontsize=12, color='tab:blue')\n",
    "    ax2.set_ylabel(\"Best Validation Accuracy\", fontsize=12, color='tab:orange')\n",
    "    ax1.set_title(\n",
    "        \"Experiment 2 (CUB-200-2011 birds)\\n\"\n",
    "        \"ResNet101V2 (ImageNet), weight decay $\\\\lambda$=0.0001 (fixed), batch size=64\\n\"\n",
    "    )\n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.legend(lines, labels, loc='best', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 用法：plot_exp2(df2, save_path=os.path.join(DRIVE_EXP_DIR,\"exp2_birds_resnet101v2.png\"))\n",
    "\n",
    "df1 = pd.read_csv(result_path1)\n",
    "plot_exp1(df1, save_path=os.path.join(DRIVE_EXP_DIR, \"exp1_birds_resnet101v2.png\"))\n",
    "\n",
    "df2 = pd.read_csv(result_path2)\n",
    "plot_exp2(df2, save_path=os.path.join(DRIVE_EXP_DIR, \"exp2_birds_resnet101v2.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
